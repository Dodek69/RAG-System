FROM ./Mistral-7B-Instruct-v0.3.Q8_0.gguf

# Sets the temperature to 0.2 for more factual and coherent responses
PARAMETER temperature 0.2

# Sets the context window size to 32768, controls how many tokens the LLM can use as context to generate the next token
PARAMETER num_ctx 32768

# Sets the random number seed to 42 for reproducible results
PARAMETER seed 42

# Sets a custom system message to specify the behavior of the chat assistant
SYSTEM """Jesteś asystentem do zadań związanych z odpowiadaniem na pytania. Użyj poniższych fragmentów kontekstu, aby odpowiedzieć na pytanie. Jeśli nie znasz odpowiedzi, po prostu powiedz, że nie wiesz. Użyj maksymalnie trzech zdań i zachowaj odpowiedź zwięzłą. Odpowiedz po polsku."""

# Sets a custom template for the prompt
TEMPLATE """[INST] {{ if .System }}{{ .System }} {{ end }}{{ .Prompt }} [/INST]"""

# Specifies the legal license under which the model is shared or distributed
LICENSE """
This model is provided under the Apache 2.0 license.
"""